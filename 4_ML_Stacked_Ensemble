import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,
    confusion_matrix, classification_report, log_loss, brier_score_loss,
    matthews_corrcoef, cohen_kappa_score
)
from sklearn.calibration import calibration_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from time import time
import os
import joblib
import warnings

warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Define directories for inputs and outputs
input_dir = r"R:\GraduateStudents\WatsonWilliamP\ML_DL_Deduc_Classification\Data"
output_dir = "stacked_model_outputs"
os.makedirs(output_dir, exist_ok=True)


# Function to load data from the original dataset using the top 20 features
def load_data(file_path=None):
    """Load data using the top 20 features from feature importance results."""
    print("Building dataset from top 20 features...")

    # Load the feature importance results
    feature_importance_path = os.path.join(input_dir, "feature_importance_agg_vs_emb.csv")
    all_scores = pd.read_csv(feature_importance_path)

    # Get top 20 features
    top_features = all_scores.head(20)['Feature'].tolist()
    print(f"Using these top 20 features: {top_features}")

    # Load the original dataset
    claim_df_path = os.path.join(input_dir, "claim_level_features.csv")
    print(f"Loading original data from {claim_df_path}...")
    claim_df = pd.read_csv(claim_df_path, low_memory=False)

    # Filter to just Aggregate and Embedded
    mask = claim_df['DEDUCTIBLE_CATEGORY_first'].isin(['Aggregate', 'Embedded'])
    filtered_df = claim_df[mask].copy()
    filtered_df['target'] = (filtered_df['DEDUCTIBLE_CATEGORY_first'] == 'Embedded').astype(int)

    # Make sure all top features exist in the dataset
    feature_columns = [col for col in top_features if col in filtered_df.columns]
    if len(feature_columns) < len(top_features):
        print(f"Warning: Only found {len(feature_columns)}/{len(top_features)} features in the dataset")
        print(f"Missing features: {set(top_features) - set(feature_columns)}")

    # Get feature data
    X = filtered_df[feature_columns].copy()
    y = filtered_df['target']

    # Print class distribution
    print("\nClass distribution:")
    print(f"Class 0 (Aggregate): {(y == 0).sum()} ({(y == 0).mean():.2%})")
    print(f"Class 1 (Embedded): {(y == 1).sum()} ({(y == 1).mean():.2%})")
    print(f"Imbalance ratio: 1:{(y == 1).sum() / (y == 0).sum():.1f}")

    # Split into train, validation, and test sets (60/20/20)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=RANDOM_STATE, stratify=y
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp
    )

    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Validation set: {X_val.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")

    return X_train, X_val, X_test, y_train, y_val, y_test, feature_columns


# Function to preprocess data
def preprocess_data(X_train, X_val, X_test):
    """Apply preprocessing to features."""
    print("Preprocessing data...")

    # Impute missing values
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train)
    X_val_imputed = imputer.transform(X_val)
    X_test_imputed = imputer.transform(X_test)

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_val_scaled = scaler.transform(X_val_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)

    # Convert back to DataFrame
    X_train_processed = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_val_processed = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)
    X_test_processed = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

    return X_train_processed, X_val_processed, X_test_processed, imputer, scaler


# Function to calculate specificity
def specificity_score(y_true, y_pred):
    """Calculate specificity (true negative rate)."""
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)


# Function to build and evaluate stacked ensemble model
def build_stacked_model(X_train, X_val, X_test, y_train, y_val, y_test):
    """Build and evaluate a stacked ensemble model."""
    print("Building stacked ensemble model...")

    # Calculate imbalance ratio for setting weights
    imbalance_ratio = (y_train == 1).sum() / (y_train == 0).sum()

    # 1. Define base models with class imbalance handling
    base_models = [
        ('rf', RandomForestClassifier(
            n_estimators=500,
            class_weight='balanced',
            n_jobs=-1,
            random_state=RANDOM_STATE
        )),
        ('gb', GradientBoostingClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=8,
            random_state=RANDOM_STATE
        )),
        ('xgb', xgb.XGBClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=10,
            scale_pos_weight=imbalance_ratio,
            n_jobs=-1,
            random_state=RANDOM_STATE
        )),
        ('lgb', lgb.LGBMClassifier(
            n_estimators=500,
            learning_rate=0.05,
            num_leaves=128,
            class_weight='balanced',
            n_jobs=-1,
            random_state=RANDOM_STATE
        ))
    ]

    # 2. Define meta-learner (final estimator)
    meta_learner = LogisticRegression(
        C=0.1,
        class_weight='balanced',
        solver='liblinear',
        random_state=RANDOM_STATE
    )

    # 3. Create and train stacked model
    stacked_model = StackingClassifier(
        estimators=base_models,
        final_estimator=meta_learner,
        cv=5,
        stack_method='predict_proba',
        n_jobs=-1
    )

    print("Training stacked model...")
    start_time = time()
    stacked_model.fit(X_train, y_train)
    training_time = time() - start_time
    print(f"Training completed in {training_time:.2f} seconds")

    # 4. Save the model
    joblib.dump(stacked_model, os.path.join(output_dir, 'stacked_model.joblib'))

    # 5. Make predictions
    y_train_pred = stacked_model.predict(X_train)
    y_val_pred = stacked_model.predict(X_val)
    y_test_pred = stacked_model.predict(X_test)

    y_train_proba = stacked_model.predict_proba(X_train)[:, 1]
    y_val_proba = stacked_model.predict_proba(X_val)[:, 1]
    y_test_proba = stacked_model.predict_proba(X_test)[:, 1]

    # 6. Calculate and store metrics
    results = {}
    results['training_time'] = training_time

    for name, y_true, y_pred, y_proba in [
        ('train', y_train, y_train_pred, y_train_proba),
        ('val', y_val, y_val_pred, y_val_proba),
        ('test', y_test, y_test_pred, y_test_proba)
    ]:
        results[f'{name}_accuracy'] = accuracy_score(y_true, y_pred)
        results[f'{name}_precision'] = precision_score(y_true, y_pred)
        results[f'{name}_recall'] = recall_score(y_true, y_pred)  # Sensitivity
        results[f'{name}_specificity'] = specificity_score(y_true, y_pred)
        results[f'{name}_f1'] = f1_score(y_true, y_pred)
        results[f'{name}_roc_auc'] = roc_auc_score(y_true, y_proba)
        results[f'{name}_avg_precision'] = average_precision_score(y_true, y_proba)
        results[f'{name}_log_loss'] = log_loss(y_true, y_proba)
        results[f'{name}_brier_score'] = brier_score_loss(y_true, y_proba)
        results[f'{name}_matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)
        results[f'{name}_cohen_kappa'] = cohen_kappa_score(y_true, y_pred)

    # 7. Print results
    print("\nStacked Model Performance:")
    print(
        f"Training - Accuracy: {results['train_accuracy']:.4f}, ROC-AUC: {results['train_roc_auc']:.4f}, F1: {results['train_f1']:.4f}")
    print(
        f"Validation - Accuracy: {results['val_accuracy']:.4f}, ROC-AUC: {results['val_roc_auc']:.4f}, F1: {results['val_f1']:.4f}")
    print(
        f"Test - Accuracy: {results['test_accuracy']:.4f}, ROC-AUC: {results['test_roc_auc']:.4f}, F1: {results['test_f1']:.4f}")
    print(
        f"Test - Precision: {results['test_precision']:.4f}, Recall: {results['test_recall']:.4f}, Specificity: {results['test_specificity']:.4f}")

    # 8. Save classification report
    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:
        f.write("Stacked Ensemble Model - Classification Report\n\n")
        f.write(classification_report(y_test, y_test_pred))

    # 9. Create and save visualizations
    # Confusion Matrix
    plt.figure(figsize=(8, 7))
    cm = confusion_matrix(y_test, y_test_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title('Stacked Model - Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'stacked_model_confusion_matrix.png'))
    plt.close()

    # ROC Curve
    plt.figure(figsize=(8, 7))
    fpr, tpr, _ = roc_curve(y_test, y_test_proba)
    plt.plot(fpr, tpr, lw=2, label=f'Stacked Model (AUC = {results["test_roc_auc"]:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Stacked Model - ROC Curve')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'stacked_model_roc_curve.png'))
    plt.close()

    # Precision-Recall Curve
    plt.figure(figsize=(8, 7))
    precision, recall, _ = precision_recall_curve(y_test, y_test_proba)
    plt.plot(recall, precision, lw=2, label=f'Stacked Model (AP = {results["test_avg_precision"]:.4f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Stacked Model - Precision-Recall Curve')
    plt.legend(loc='best')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'stacked_model_pr_curve.png'))
    plt.close()

    # Calibration Curve
    plt.figure(figsize=(8, 7))
    prob_true, prob_pred = calibration_curve(y_test, y_test_proba, n_bins=10)
    plt.plot(prob_pred, prob_true, 's-', label='Stacked Model')
    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
    plt.xlabel('Mean predicted probability')
    plt.ylabel('Fraction of positives')
    plt.title('Stacked Model - Calibration Curve')
    plt.legend(loc='best')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'stacked_model_calibration_curve.png'))
    plt.close()

    # 10. Save results to CSV
    results_df = pd.DataFrame({
        'Metric': [
            'Accuracy', 'Precision', 'Recall (Sensitivity)', 'Specificity', 'F1 Score',
            'ROC AUC', 'Average Precision', 'Log Loss', 'Brier Score',
            'Matthews Correlation', 'Cohen Kappa', 'Training Time (s)'
        ],
        'Train': [
            results['train_accuracy'], results['train_precision'], results['train_recall'],
            results['train_specificity'], results['train_f1'], results['train_roc_auc'],
            results['train_avg_precision'], results['train_log_loss'], results['train_brier_score'],
            results['train_matthews_corrcoef'], results['train_cohen_kappa'], results['training_time']
        ],
        'Validation': [
            results['val_accuracy'], results['val_precision'], results['val_recall'],
            results['val_specificity'], results['val_f1'], results['val_roc_auc'],
            results['val_avg_precision'], results['val_log_loss'], results['val_brier_score'],
            results['val_matthews_corrcoef'], results['val_cohen_kappa'], '-'
        ],
        'Test': [
            results['test_accuracy'], results['test_precision'], results['test_recall'],
            results['test_specificity'], results['test_f1'], results['test_roc_auc'],
            results['test_avg_precision'], results['test_log_loss'], results['test_brier_score'],
            results['test_matthews_corrcoef'], results['test_cohen_kappa'], '-'
        ]
    })

    results_df.to_csv(os.path.join(output_dir, 'stacked_model_results.csv'), index=False)

    # Return model, results, and base models for further analysis
    return stacked_model, results, base_models


# Function to create learning curves
def create_learning_curves(model, X, y):
    """Create and save learning curves for the model."""
    # Set up the figure
    plt.figure(figsize=(10, 8))

    # Calculate learning curve
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y,
        cv=5,
        scoring='roc_auc',
        train_sizes=np.linspace(0.1, 1.0, 10),
        random_state=RANDOM_STATE,
        n_jobs=-1
    )

    # Calculate mean and std for training and test scores
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    # Plot learning curve
    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')

    plt.plot(train_sizes, test_mean, 'o-', color='red', label='Cross-validation Score')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')

    plt.title('Learning Curve - Stacked Ensemble (ROC AUC)')
    plt.xlabel('Training Examples')
    plt.ylabel('ROC AUC Score')
    plt.grid(True)
    plt.legend(loc='best')
    plt.tight_layout()

    # Save the figure
    plt.savefig(os.path.join(output_dir, 'learning_curve.png'))
    plt.close()


# Function to analyze predictions
def analyze_predictions(model, X_test, y_test):
    """Analyze predictions to gain insights."""
    # Get predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # Create a DataFrame with true labels and predictions
    pred_df = pd.DataFrame({
        'True_Label': y_test,
        'Predicted_Label': y_pred,
        'Predicted_Probability': y_proba
    })

    # Save predictions for further analysis
    pred_df.to_csv(os.path.join(output_dir, 'predictions.csv'), index=False)

    # Analyze prediction errors
    pred_df['Error'] = (pred_df['True_Label'] != pred_df['Predicted_Label']).astype(int)

    # Group by true label and calculate error rates
    error_rates = pred_df.groupby('True_Label')['Error'].mean().reset_index()
    error_rates.columns = ['True_Label', 'Error_Rate']

    # Save error analysis
    error_rates.to_csv(os.path.join(output_dir, 'error_analysis.csv'), index=False)

    # Plot error analysis by class
    plt.figure(figsize=(8, 6))
    sns.barplot(x='True_Label', y='Error_Rate', data=error_rates)
    plt.title('Error Rate by Class')
    plt.xlabel('True Class')
    plt.ylabel('Error Rate')
    plt.xticks([0, 1], ['Aggregate', 'Embedded'])
    plt.grid(axis='y')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'error_by_class.png'))
    plt.close()

    # Plot probability distribution by true class
    plt.figure(figsize=(10, 6))
    for class_value, label in [(0, 'Aggregate'), (1, 'Embedded')]:
        class_probs = pred_df[pred_df['True_Label'] == class_value]['Predicted_Probability']
        sns.kdeplot(class_probs, label=f'True Class: {label}')

    plt.title('Predicted Probability Distribution by True Class')
    plt.xlabel('Predicted Probability of Embedded Class')
    plt.ylabel('Density')
    plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'probability_distribution.png'))
    plt.close()


# Main execution function for stacked model
def run_stacked_ensemble_analysis(data_file=None):
    """Run stacked ensemble analysis and evaluation."""

    # 1. Load the pre-split datasets
    print("Loading and preparing data...")
    X_train, X_val, X_test, y_train, y_val, y_test, features = load_data(data_file)

    print(
        f"Training set: {X_train.shape[0]} samples, Validation set: {X_val.shape[0]} samples, Test set: {X_test.shape[0]} samples")

    # 2. Preprocess data
    print("\nPreprocessing data...")
    X_train_processed, X_val_processed, X_test_processed, imputer, scaler = preprocess_data(
        X_train, X_val, X_test
    )

    # Save preprocessors
    joblib.dump(imputer, os.path.join(output_dir, 'imputer.joblib'))
    joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))

    # 3. Build and evaluate stacked model
    stacked_model, results, base_models = build_stacked_model(
        X_train_processed, X_val_processed, X_test_processed,
        y_train, y_val, y_test
    )

    # 4. Extra analysis: learning curves
    print("\nGenerating learning curves...")
    create_learning_curves(stacked_model, X_train_processed, y_train)

    # 5. Prediction analysis
    print("Analyzing predictions...")
    analyze_predictions(stacked_model, X_test_processed, y_test)

    return stacked_model, results


# Execute the analysis if run as a script
if __name__ == "__main__":
    stacked_model, results = run_stacked_ensemble_analysis()
    print("\nStacked ensemble analysis complete!")

    # Print key test metrics
    print("\nStacked Model Test Performance:")
    print(f"Accuracy: {results['test_accuracy']:.4f}")
    print(f"ROC AUC: {results['test_roc_auc']:.4f}")
    print(f"F1 Score: {results['test_f1']:.4f}")
    print(f"Precision: {results['test_precision']:.4f}")
    print(f"Recall (Sensitivity): {results['test_recall']:.4f}")
    print(f"Specificity: {results['test_specificity']:.4f}")
