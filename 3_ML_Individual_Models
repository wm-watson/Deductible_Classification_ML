import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,
    confusion_matrix, classification_report, log_loss, brier_score_loss,
    matthews_corrcoef, cohen_kappa_score
)
from sklearn.calibration import calibration_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from time import time
import os
import joblib
import warnings

warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Define directory to save outputs
output_dir = "model_outputs"
os.makedirs(output_dir, exist_ok=True)


# Function to load data - replace with your data loading logic
def load_data(file_path):
    """Load data from file and split features and target."""
    # For this example, we'll create synthetic data
    # Replace this with your actual data loading code

    # If you have a CSV file, uncomment this:
    # df = pd.read_csv(file_path)
    # X = df.drop('target', axis=1)
    # y = df['target']

    # Creating synthetic data for demonstration
    from sklearn.datasets import make_classification
    X, y = make_classification(
        n_samples=10000,
        n_features=20,
        n_informative=10,
        n_redundant=5,
        random_state=RANDOM_STATE
    )
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    X = pd.DataFrame(X, columns=feature_names)
    y = pd.Series(y, name='target')

    return X, y


# Function to preprocess data
def preprocess_data(X_train, X_val, X_test):
    """Apply preprocessing to features."""
    # Impute missing values
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train)
    X_val_imputed = imputer.transform(X_val)
    X_test_imputed = imputer.transform(X_test)

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_val_scaled = scaler.transform(X_val_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)

    # Convert back to DataFrame
    X_train_processed = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_val_processed = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)
    X_test_processed = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

    return X_train_processed, X_val_processed, X_test_processed, imputer, scaler


# Function to evaluate a model
def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_name):
    """Evaluate model with comprehensive metrics."""
    results = {}
    start_time = time()

    # Train the model
    model.fit(X_train, y_train)
    training_time = time() - start_time
    results['training_time'] = training_time

    # Make predictions
    start_time = time()
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    y_test_pred = model.predict(X_test)

    y_train_proba = model.predict_proba(X_train)[:, 1]
    y_val_proba = model.predict_proba(X_val)[:, 1]
    y_test_proba = model.predict_proba(X_test)[:, 1]
    prediction_time = time() - start_time
    results['prediction_time'] = prediction_time

    # Calculate metrics for each dataset
    datasets = {
        'train': (y_train, y_train_pred, y_train_proba),
        'val': (y_val, y_val_pred, y_val_proba),
        'test': (y_test, y_test_pred, y_test_proba)
    }

    for dataset_name, (y_true, y_pred, y_proba) in datasets.items():
        results[f'{dataset_name}_accuracy'] = accuracy_score(y_true, y_pred)
        results[f'{dataset_name}_precision'] = precision_score(y_true, y_pred)
        results[f'{dataset_name}_recall'] = recall_score(y_true, y_pred)
        results[f'{dataset_name}_f1'] = f1_score(y_true, y_pred)
        results[f'{dataset_name}_roc_auc'] = roc_auc_score(y_true, y_proba)
        results[f'{dataset_name}_avg_precision'] = average_precision_score(y_true, y_proba)
        results[f'{dataset_name}_log_loss'] = log_loss(y_true, y_proba)
        results[f'{dataset_name}_brier_score'] = brier_score_loss(y_true, y_proba)
        results[f'{dataset_name}_matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)
        results[f'{dataset_name}_cohen_kappa'] = cohen_kappa_score(y_true, y_pred)

        # Confusion matrix components
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        results[f'{dataset_name}_true_negatives'] = tn
        results[f'{dataset_name}_false_positives'] = fp
        results[f'{dataset_name}_false_negatives'] = fn
        results[f'{dataset_name}_true_positives'] = tp
        results[f'{dataset_name}_specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0

    # Print key results
    print(f"\n{model_name} Results:")
    print(f"Training Time: {training_time:.2f} seconds")
    print(f"Validation Accuracy: {results['val_accuracy']:.4f}")
    print(f"Validation ROC AUC: {results['val_roc_auc']:.4f}")
    print(f"Validation F1 Score: {results['val_f1']:.4f}")
    print(f"Test Accuracy: {results['test_accuracy']:.4f}")
    print(f"Test ROC AUC: {results['test_roc_auc']:.4f}")
    print(f"Test F1 Score: {results['test_f1']:.4f}")

    # Create visualizations
    create_model_visualizations(
        model_name,
        y_test,
        y_test_pred,
        y_test_proba,
        model,
        X_test
    )

    return model, results


# Function to create visualizations
def create_model_visualizations(model_name, y_true, y_pred, y_pred_proba, model, X_test):
    """Create and save visualizations for model evaluation."""
    # Create a figure with multiple subplots
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle(f"{model_name} Performance Evaluation", fontsize=16)

    # 1. Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[0, 0])
    axes[0, 0].set_title("Confusion Matrix")
    axes[0, 0].set_xlabel("Predicted Label")
    axes[0, 0].set_ylabel("True Label")

    # 2. ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    axes[0, 1].plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.3f})')
    axes[0, 1].plot([0, 1], [0, 1], 'k--')
    axes[0, 1].set_xlim([0.0, 1.0])
    axes[0, 1].set_ylim([0.0, 1.05])
    axes[0, 1].set_xlabel('False Positive Rate')
    axes[0, 1].set_ylabel('True Positive Rate')
    axes[0, 1].set_title('Receiver Operating Characteristic')
    axes[0, 1].legend(loc="lower right")

    # 3. Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    avg_precision = average_precision_score(y_true, y_pred_proba)
    axes[1, 0].plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')
    axes[1, 0].set_xlabel('Recall')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].set_title('Precision-Recall Curve')
    axes[1, 0].legend(loc="lower left")

    # 4. Calibration Curve
    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=10)
    axes[1, 1].plot(prob_pred, prob_true, marker='o', linewidth=1)
    axes[1, 1].plot([0, 1], [0, 1], 'k--')
    axes[1, 1].set_xlabel('Mean Predicted Probability')
    axes[1, 1].set_ylabel('Fraction of Positives')
    axes[1, 1].set_title('Calibration Curve')

    # Adjust layout and save
    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for the suptitle
    plt.savefig(os.path.join(output_dir, f"{model_name.replace(' ', '_').lower()}_evaluation.png"))
    plt.close()

    # Feature importance (if available)
    try:
        if hasattr(model, 'feature_importances_'):
            # Get feature importances
            importances = model.feature_importances_
            feature_names = X_test.columns
            indices = np.argsort(importances)[::-1]

            # Plot feature importances
            plt.figure(figsize=(12, 8))
            plt.title(f'Feature Importances: {model_name}')
            plt.bar(range(min(20, len(indices))), importances[indices[:20]], align='center')
            plt.xticks(range(min(20, len(indices))), [feature_names[i] for i in indices[:20]], rotation=90)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f"{model_name.replace(' ', '_').lower()}_feature_importance.png"))
            plt.close()
    except:
        print(f"Could not generate feature importance plot for {model_name}")


# Main execution function
def run_ensemble_models(data_file='data.csv'):
    """Run and evaluate multiple ensemble models."""

    # 1. Load and split data
    print("Loading and preparing data...")
    X, y = load_data(data_file)

    # Split into train, validation, and test sets (60/20/20)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=RANDOM_STATE, stratify=y
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp
    )

    print(
        f"Training set: {X_train.shape[0]} samples, Validation set: {X_val.shape[0]} samples, Test set: {X_test.shape[0]} samples")

    # 2. Preprocess data
    print("Preprocessing data...")
    X_train_processed, X_val_processed, X_test_processed, imputer, scaler = preprocess_data(
        X_train, X_val, X_test
    )

    # Save preprocessors
    joblib.dump(imputer, os.path.join(output_dir, 'imputer.joblib'))
    joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))

    # 3. Define models
    models = {
        'Random Forest': RandomForestClassifier(
            n_estimators=1000,
            max_depth=None,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features='sqrt',
            bootstrap=True,
            n_jobs=-1,
            random_state=RANDOM_STATE
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            max_depth=8,
            min_samples_split=2,
            min_samples_leaf=1,
            subsample=0.8,
            random_state=RANDOM_STATE
        ),
        'XGBoost': xgb.XGBClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            max_depth=10,
            min_child_weight=1,
            gamma=0,
            subsample=0.8,
            colsample_bytree=0.8,
            objective='binary:logistic',
            scale_pos_weight=1,
            tree_method='hist',
            n_jobs=-1,
            random_state=RANDOM_STATE
        ),
        'LightGBM': lgb.LGBMClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            num_leaves=256,
            max_depth=-1,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            n_jobs=-1,
            random_state=RANDOM_STATE
        ),
        'CatBoost': cb.CatBoostClassifier(
            iterations=1000,
            learning_rate=0.05,
            depth=10,
            loss_function='Logloss',
            verbose=False,
            random_seed=RANDOM_STATE
        )
    }

    # 4. Train and evaluate each model
    all_results = {}
    trained_models = {}

    for model_name, model in models.items():
        print(f"\nTraining {model_name}...")
        trained_model, model_results = evaluate_model(
            model,
            X_train_processed, X_val_processed, X_test_processed,
            y_train, y_val, y_test,
            model_name
        )

        all_results[model_name] = model_results
        trained_models[model_name] = trained_model

        # Save model
        joblib.dump(trained_model, os.path.join(output_dir, f"{model_name.replace(' ', '_').lower()}.joblib"))

    # 5. Create comparison of all models
    results_df = pd.DataFrame({
        model_name: {
            metric: values[f'test_{metric}']
            for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'avg_precision',
                           'log_loss', 'brier_score', 'matthews_corrcoef', 'cohen_kappa',
                           'specificity']
        }
        for model_name, values in all_results.items()
    })

    results_df = results_df.transpose()

    # Add training time
    results_df['training_time'] = [all_results[model]['training_time'] for model in results_df.index]

    # Save results
    results_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'))

    # Create comparison visualizations
    create_comparison_visualizations(results_df, X_test_processed, y_test, trained_models)

    return results_df, trained_models


# Function to create comparison visualizations
def create_comparison_visualizations(results_df, X_test, y_test, models):
    """Create visualizations comparing all models."""

    # 1. Performance metrics comparison
    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

    # Plot bar chart for each metric
    plt.figure(figsize=(15, 10))

    for i, metric in enumerate(metrics_to_plot):
        plt.subplot(2, 3, i + 1)
        results_df[metric].sort_values().plot(kind='bar', color='skyblue')
        plt.title(f'{metric.replace("_", " ").title()}')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

    plt.savefig(os.path.join(output_dir, 'metrics_comparison.png'))
    plt.close()

    # 2. ROC curves comparison
    plt.figure(figsize=(12, 10))

    for model_name, model in models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves Comparison')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(output_dir, 'roc_curves_comparison.png'))
    plt.close()

    # 3. Precision-Recall curves comparison
    plt.figure(figsize=(12, 10))

    for model_name, model in models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        avg_precision = average_precision_score(y_test, y_pred_proba)
        plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curves Comparison')
    plt.legend(loc='lower left')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(output_dir, 'pr_curves_comparison.png'))
    plt.close()

    # 4. Training time comparison
    plt.figure(figsize=(12, 6))
    results_df['training_time'].sort_values().plot(kind='bar', color='lightgreen')
    plt.title('Training Time Comparison (seconds)')
    plt.ylabel('Time (seconds)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'training_time_comparison.png'))
    plt.close()


# Execute the analysis
if __name__ == "__main__":
    results_df, trained_models = run_ensemble_models()
    print("\nAnalysis complete. Results saved to:", output_dir)

    # Display final results summary
    print("\nFinal Model Comparison (Test Set Metrics):")
    display_cols = ['accuracy', 'roc_auc', 'f1', 'precision', 'recall']
    print(results_df[display_cols].sort_values('roc_auc', ascending=False))
